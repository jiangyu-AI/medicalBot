 https://medium.com/towards-data-science/tensor2tensor-and-one-model-to-learn-them-all-7ef3f9b61ba4

 https://github.com/tensorflow/tensor2tensor

 https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html
 https://github.com/facebookresearch/fastText
 https://github.com/fchollet/keras/tree/master/tests
 :wq
 cross-lingual knowlege linking across wiki knowlege bases
 enriching the crosslingual link structure of wikipedia - a classification-based approach
 cross-languang and crosws-encylopedia article linking using misxed-language topic model and hypernym translation
 
 you can store the output of the program like this suppose, object file name is a, run the following command to save output in a file:

 ./a > file.txt



screen size: 
C_a :resize -h 87
focusminsize [ ( width|max|_ ) ( height|max|_ ) ]
COLUMNS=87
LINES=47;
export COLUMNS LINES;

TencentW3chat@US?
Tencent-Cafelife10cent4guest?
X-WingSpace@123
x-wing-Guest:Guest#123

vim open multiple file in vertical splits: 
vim -O *

Use Jupyter notebook remotely
First, make sure you install Jupyter notebook in both remote (working station in your offcie) and local (your home computer)

In remote host, open the terminal, change directory to where you have your notebooks and type:

jupyter notebook --no-browser --port=8889

# you should leave the this open

In your local computer, open MS-DOS cmd (if using Windows) or Unix terminal, then type:

ssh -N -f -L localhost:8888:localhost:8889 jyu@10.96.97.220

# make sure to change `username` to your real username in remote host
# change `your_remote_host_name` to your address of your working station
# Example: ssh -N -f -L localhost:8888:localhost:8889 laura@cs.rutgers.edu

Now open web browser (google chrome, firefox, ...) and type:

localhost:8888
# you will see your notebooks in your given directory




Expire Account

Let the account expire to disallowing a user from logging in from any source including ssh:
# disallow peter from logging in
sudo usermod --expiredate 1 peter

This is how you can reenable that account:
# set expiration date of peter to Never
sudo usermod --expiredate "" tencentnlp



conda --version
conda update conda
conda create -n bunnies python=3.5
conda info --envs
source activate bunnies
source deactivate
conda create -n flowers --clone snowflakes
conda remove -n flowers --all
conda search --full-name python
conda create -n snakes python=3.6.2
conda list
conda search beautifulsoup4
conda install -n bunnies beautifulsoup4
conda install --channel https://anaconda.org/pandas bottleneck 
pip --version
pip install -n snakes see
pip install see
conda remove --name root see
conda remove -n snakes python
conda env export > enviroment.yml
conda env create -f enviroment.yml
conda update python
conda list | grep pytorch
conda install pytorch-gpu



unzip file.zip -d destination_folder

wiki translation dump 
https://dumps.wikimedia.org/other/contenttranslation/20170721/

wiki json dump for elastic search: 
https://dumps.wikimedia.org/other/cirrussearch/20170724/



buid Chinese to English dictionary from wikidata using English label and Chinese label: 
/home/jyu/data/wiki/labels.txt    in total, there are 940868 almost a million entries having both Chinese and English labels


ctr u   half page up
ctr d   half page down
ctr f  page forward
ctr b  page back

#  previous same word
*  next same word
Compress an Entire Directory or a Single File
tar -czvf name-of-archive.tar.gz /path/to/directory-or-file
tar -czvf archive.tar.gz /home/ubuntu --exclude=/home/ubuntu/Downloads --exclude=/home/ubuntu/.cache

Exclude Directories and Files
tar -xzvf archive.tar.gz -C /tmp
tar -xvzf yourfile.tar.gz
above is vim notes



wiki data: 
index file has lines
    offset1:pageId1:title1
    offset1:pageId2:title2

get wikipedia link: 
https://en.wikipedia.org/wiki/  +  title value getting from the following: 

https://en.wikipedia.org/wiki/Scotland
"sitelinks": {
	"enwiki": {
		    "site": "enwiki",
		    "title": "Scotland",
		    "badges": [
			"Q17437798"
		    ]
		},
}



number of files from bfs one level: 
78977 files
(918 entry point for multi meaning entries are dropped )

location:
@228 
/home/jyu/data/baike/jsonFiles/bfsOneLevel

all baike science and entries from bfs one level in compressed: 
size: 3.0 GB
/home/jyu/data/baike/baike.tar.gz




1st floor printer: 
10.3.5.52



17765

航空航天 
(共463个)汽车工程 
(共1341个)生物医学 
(共251个)环境科学 
(共497个)气象科技 
(共295个)水产养殖 
(共1479个)食品科技 
(共1075个)通信科技 
(共3634个)水利科学 
(共1382个)核能利用 
(共1351个)体育科学 
(共1391个)力学 
(共1042个)化工科技 
(共1754个)电子信息 
(共1810个)心理健康 
(共935个)



du -hs /path/to/directory
baike science data is completed: 
location: 228   /home/jyu/data/baike/ 
number of files:  40605
size: 3.4G (html)  316M jsonfiles





num of files in science:
[463, 1340, 251, 497, 295, 1478, 1072, 3630, 1382, 1350, 1388, 1040, 1754, 1807, 933, 7251, 8155, 4105, 2417]
>>> sum(nums)
40608
jyu@yu:~/workspace/medicalBot$ find ~/data/baikeFive/science/. -type f | wc -l
40662

find /home/jyu/data/baikeFive/jsonFiles0/. -type f |wc -l
40605

Each fold has six files(3 .txt, 3 hidden files)


num of files in bfs new files: 
wc -l /home/jyu/data/baikeFive/urlsBfsNew.txt
78975 /home/jyu/data/baikeFive/urlsBfsNew.txt



matches between baike names and wiki labels

413 matches_by_nameEn.txt
1638 matches_by_nameCh.txt
matches ratio: by nameCh
>>> 1638/24382
0.0671807070789927

baike sstats:
count_total: 24382 baikeMedical/names_id.txt
ratio having Englsh name: < 6%

wiki stats:
count_total: 27827809 wiki/labels_id.txt
count_label_zh: 1528208
count_label_en: 16616047
ratio having label_zh: 0.05491657446274069
ratio having label_en: 0.5971022153737574






great Traditional Chinese and simplified Chinese converter: 

https://pypi.python.org/pypi/hanziconv


# medicalBot
5 July: map id between wiki data and baiduBaike data



install virtualenvwrapper: 

http://exponential.io/blog/2015/02/10/install-virtualenv-and-virtualenvwrapper-on-ubuntu/

Step-by-step instructions

Open a terminal and install the following packages.

sudo apt-get install python-pip python-dev build-essential

sudo pip install virtualenv virtualenvwrapper

sudo pip install --upgrade pip

Setup virtualenvwrapper in ~/.bashrc.

# Create a backup of your .bashrc file
cp ~/.bashrc ~/.bashrc-org

# Be careful with this command
printf '\n%s\n%s\n%s' '# virtualenv' 'export WORKON_HOME=~/virtualenvs' \
'source /usr/local/bin/virtualenvwrapper.sh' >> ~/.bashrc

Enable the virtual environment.

source ~/.bashrc

mkdir -p $WORKON_HOME

mkvirtualenv api

# Exit the 'api' virtual environment
deactivate

Tips on using virtualenv

To enable the api virtual environment, run the following command:

workon api

To deactivate the api virtual environment, run the following command:

deactivate




1, crawl and parse the following four websites for medical pages. 
http://baike.baidu.com/wikitag/taglist?tagId=75953
科学百科疾病症状分类(共7252个)

http://baike.baidu.com/wikitag/taglist?tagId=75954
科学百科药物分类(共8155个)

http://baike.baidu.com/wikitag/taglist?tagId=75956
科学百科中医药分类(共4105个)

http://baike.baidu.com/wikitag/taglist?tagId=75955
科学百科诊疗方法分类(共2418个)

In total: 21930

2,

http://python-guide-pt-br.readthedocs.io/en/latest/dev/virtualenvs/
count number of files: 
grep -Rl "curl" ./ | wc -l

data json file format:
{"title":{
         "summary":"...",
         "cause":"...",
         "treatment":"..."
    }
}

{"data":[{"title":"XXX", "paragraphs":[{"context":"", "qas":[
    {"answers":[{"answer_start":255, "text":"west"}], "question":"YYY?", "id":"5735cc33012e2f140011a069"}, 
    {"answers":[{"answer_start":255, "text":"west"}], "question":"YYY?", "id":"5735cc33012e2f140011a069"}, 
    
    ]}]}]}

// solution for writing and displaying Chinase
Setting the Default Java File Encoding to UTF-8:

    export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8

server.228

git
jira

codeReview
unitTest
